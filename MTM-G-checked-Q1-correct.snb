{"metadata":{"name":"MTM-G-checked-Q1","user_save_timestamp":"1190-11-11T11:00:00.000Z","auto_save_timestamp":"19719700-11-11T11:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null},"cells":[{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"9265623F82D44A268E1C72C4FD7F48B6"},"cell_type":"code","source":":markdown\n#NamedVector.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res1: String = #NamedVector.scala\n"},{"metadata":{},"data":{"text/markdown":"#NamedVector.scala"},"output_type":"execute_result","execution_count":1,"time":"Took: 4 seconds 983 milliseconds, at 2017-6-14 16:26"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"2B8FFFF2610F40BA873138C331219D3E"},"cell_type":"code","source":"\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\n//import org.apache.spark.util.Vector\nimport org.apache.spark.mllib.linalg.DenseVector\n//import org.apache.spark.ml.linalg.{Vector, Vectors}\n//import org.apache.spark.mllib.linalg.{Vectors, Vector}\n//package org.lipn.som.global\n\nimport java.util.concurrent.TimeUnit._\nimport org.apache.spark.rdd.RDD\n//import org.apache.spark.util.Vector\nimport scala.concurrent.duration.{FiniteDuration, Duration}\n//import org.lipn.som.som.pointObj\nimport org.apache.spark.util\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.DenseVector\nimport java.util.concurrent.TimeUnit._\nimport org.apache.spark.rdd.RDD\nimport scala.concurrent.duration.{FiniteDuration, Duration}\nimport org.apache.spark.util\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":2,"time":"Took: 5 seconds 84 milliseconds, at 2017-6-14 16:26"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"7128594164A24452A9A28D1190CCB97D"},"cell_type":"code","source":"//package org.lipn.som.utils\n\n//import org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.mllib.linalg.DenseVector\n\n\n/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 27/03/13\n * Time: 17:07\n * To change this template use File | Settings | File Templates.\n */\nclass NamedVector(elements: Array[Double], val cls: Int) extends DenseVector(elements) with Serializable {\n  override def toString(): String = {\n    \"#\"+cls+\" \"+super.toString()\n  }\n  def toJSON(clusterId: Int): String = {\n    var str = new StringBuilder\n    str append \"{\"\n    for (i <- 0 until elements.length) {\n      str append \"attr\"+i+\":\"+elements(i)+\", \"\n    }\n    str append \"cls:\\\"\"+cls+\"\\\", \"\n    str append \"clusterId:\"+clusterId\n    str append \"}\\n\"\n    str.toString()\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.linalg.DenseVector\ndefined class NamedVector\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":3,"time":"Took: 4 seconds 281 milliseconds, at 2017-6-14 16:26"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"2736AD3B4D7749D886A123CA1D209170"},"cell_type":"code","source":":markdown\n#Matrix.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res5: String = #Matrix.scala\n"},{"metadata":{},"data":{"text/markdown":"#Matrix.scala"},"output_type":"execute_result","execution_count":4,"time":"Took: 5 seconds 393 milliseconds, at 2017-6-14 16:26"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"7CDF2C41EAC5425782484B5FC1B9D70B"},"cell_type":"code","source":"//package org.lipn.som.utils\n\n/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 27/03/13\n * Time: 19:10\n * To change this template use File | Settings | File Templates.\n */\nclass Matrix(val elements: Array[Array[Double]]) extends Serializable {\n  //def this(nbRow: Int, nbCol: Int) = this(Array.tabulate(nbRow, nbCol){case (r, c) => r+c})\n  def this(nbElements: Int, initValue: Double) = this(Array.fill(nbElements, nbElements)(initValue))\n\n  def fillRow(row: Int, value: Double) = {elements(row) = Array.fill(elements.length)(value)}\n  def apply(row: Int, col: Int):Double = elements(row)(col)\n\n  def addel(row: Int, col: Int, value: Double) {elements(row)(col) += value}\n\n  def += (other: Matrix): Matrix = {\n    //todo: add length check\n    //if (length != other.length)\n    //  throw new IllegalArgumentException(\"Matrix of different length\")\n    var ans = 0.0\n    var i = 0\n    for (i <- 0 until elements.length) {\n      for (j <- 0 until elements(i).length) {\n        elements(i)(j) += other.elements(i)(j)\n      }\n    }\n    this\n  }\n\n  def /= (other: Matrix): Matrix = {\n    //todo: add length check\n    //if (length != other.length)\n    //  throw new IllegalArgumentException(\"Matrix of different length\")\n    var ans = 0.0\n    var i = 0\n    for (i <- 0 until elements.length) {\n      for (j <- 0 until elements(i).length) {\n        if (other.elements(i)(j) == 0) elements(i)(j) /= Double.MinPositiveValue\n        else elements(i)(j) /= other.elements(i)(j)\n      }\n    }\n    this\n  }\n\n  /*def strRow(rowId: Int): String = {\n    var first = true\n\n    var str = new StringBuilder()\n    str append \"[\"\n    for (x <- elements(rowId)) {\n      if (first) {\n        first = false\n      }\n      else {\n        str append \", \"\n      }\n      str append x\n    }\n    str append \"]\"\n    str.toString()\n  }\n\n  def strCol(colId: Int): String = {\n    var first = true\n\n    var str = new StringBuilder()\n    str append \"[\"\n    for (row <- elements) {\n      if (first) {\n        str append row(colId)\n        first = false\n      }\n      else {\n        str append \", \"\n        str append row(colId)\n      }\n    }\n    str append \"]\"\n    str.toString()\n  }*/\n\n  //override def toString = elements.mkString(\"|\", \" | \", \"|\")\n  override def toString: String = {\n    val str = new StringBuilder()\n    for (row <- elements) {\n      for (elem <- row) {\n        str.append(\" |\").append(\"%.2f\".format(elem))\n      }\n      str.append(\"|\\n\")\n    //elements.foreach{r =>\n      //str append row.mkString(\"|\", \" | \", \"|\")+\"\\n\"\n    }\n    str.toString()\n  }\n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"defined class Matrix\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":5,"time":"Took: 8 seconds 288 milliseconds, at 2017-6-14 16:26"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"87E58274AD6345E4B90AF097F294602F"},"cell_type":"code","source":":markdown\n#IO.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res8: String = #IO.scala\n"},{"metadata":{},"data":{"text/markdown":"#IO.scala"},"output_type":"execute_result","execution_count":6,"time":"Took: 6 seconds 588 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B842BCF3F492457B8A762B8B3CB174CC"},"cell_type":"code","source":"//package org.lipn.som.utils\n\nimport java.io.File\n\n/**\n * Company : Altic - LIPN\n * User: Tugdual Sarazin\n * Date: 06/06/14\n * Time: 11:48\n */\nobject IO {\n\n  /** Deletes each file or directory (recursively) in `files`.*/\n  def delete(files: Iterable[File]): Unit = files.foreach(delete)\n\n  /** Deletes `file`, recursively if it is a directory. */\n  def delete(file: File)\n  {\n      val deleted = file.delete()\n      if(!deleted && file.isDirectory)\n      {\n        delete(listFiles(file))\n        file.delete\n      }\n  }\n\n  /** Returns the children of directory `dir` that match `filter` in a non-null array.*/\n  def listFiles(filter: java.io.FileFilter)(dir: File): Array[File] = wrapNull(dir.listFiles(filter))\n\n  /** Returns the children of directory `dir` that match `filter` in a non-null array.*/\n  def listFiles(dir: File, filter: java.io.FileFilter): Array[File] = wrapNull(dir.listFiles(filter))\n\n  /** Returns the children of directory `dir` in a non-null array.*/\n  def listFiles(dir: File): Array[File] = wrapNull(dir.listFiles())\n\n  private def wrapNull(a: Array[File]) =\n    if(a == null)\n      new Array[File](0)\n    else\n      a\n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"import java.io.File\ndefined object IO\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":7,"time":"Took: 8 seconds 286 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"328A3B91F45A4B65AFB7357CD85B9A6D"},"cell_type":"code","source":"//package org.lipn.som.utils\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.rdd.RDD\n\n/**\n * Company : Altic - LIPN\n * User: Tugdual Sarazin\n * Date: 07/01/14\n * Time: 12:37\n */\nobject SparkReader {\n  def parse(sc: SparkContext, filePath: String, splitRegex: String): RDD[NamedVector] = {\n    sc.textFile(filePath).map{line =>\n      val arrayDouble = line.split(splitRegex).map(_.toDouble)\n      new NamedVector(arrayDouble.dropRight(1), arrayDouble.last.toInt)\n    }\n  }\n}","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.SparkContext\nimport org.apache.spark.rdd.RDD\ndefined object SparkReader\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":8,"time":"Took: 9 seconds 331 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F39D6EFB33AF498D8AE47212D8D7CB83"},"cell_type":"code","source":":markdown\nSparkReader.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res12: String = SparkReader.scala\n"},{"metadata":{},"data":{"text/markdown":"SparkReader.scala"},"output_type":"execute_result","execution_count":9,"time":"Took: 7 seconds 913 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B29DB159843A4CA685233DC832AB8E1B"},"cell_type":"code","source":":markdown\n#NmiMetric.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res14: String = #NmiMetric.scala\n"},{"metadata":{},"data":{"text/markdown":"#NmiMetric.scala"},"output_type":"execute_result","execution_count":10,"time":"Took: 7 seconds 482 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B5027B8CE462421B85E62425AC56ADC8"},"cell_type":"code","source":"//package org.lipn.som.utils\n\n/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 24/05/13\n * Time: 19:02\n * To change this template use File | Settings | File Templates.\n */\n//object NmiMetric extends App {\nobject NmiMetric {\n  def jointProbabilty(x: Array[Int], y: Array[Int]) {\n\n  }\n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"defined object NmiMetric\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":11,"time":"Took: 4 seconds 778 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"EC59E17DA3FD4582857637194E20BFBE"},"cell_type":"code","source":":markdown\n#AbstractPrototype.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res17: String = #AbstractPrototype.scala\n"},{"metadata":{},"data":{"text/markdown":"#AbstractPrototype.scala"},"output_type":"execute_result","execution_count":12,"time":"Took: 4 seconds 846 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"74AF2500F11D4470B975946BAAEE9254"},"cell_type":"code","source":"//package org.lipn.som.global\nimport org.apache.spark.mllib.linalg.{Vectors, Vector, DenseVector}\n\n//import org.apache.spark.util\n\n/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 14/06/13\n * Time: 12:42\n * To change this template use File | Settings | File Templates.\n */\nabstract class AbstractPrototype(val id: Int, var _point: DenseVector) extends Serializable {\n  def update(newPoint: DenseVector): Double = {\n //   val dist = _point.dist(newPoint)\n  val dist = Vectors.sqdist(_point, newPoint)\n\n    _point = newPoint\n    dist\n  }\n\n // def dist(data: Vector) = _point.dist(data) // a modifier: - ajouter une pondÃ©ration fixe; - ajouter une pondÃ©ration adaptative\ndef dist(data: DenseVector) = Vectors.sqdist(_point,data) // a modifier: - ajouter une pondÃ©ration fixe; - ajouter une pondÃ©ration adaptative\n\n//def dist(prototype: AbstractPrototype) = _point.dist(prototype._point)\ndef dist(prototype: AbstractPrototype) = Vectors.sqdist(_point,prototype._point)\n  \n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.linalg.{Vectors, Vector, DenseVector}\ndefined class AbstractPrototype\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":13,"time":"Took: 2 seconds 667 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"0BD5BA82938F41DD8C42AECAD28ED107"},"cell_type":"code","source":":markdown \n#AbstractModel.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res20: String = #AbstractModel.scala\n"},{"metadata":{},"data":{"text/markdown":"#AbstractModel.scala"},"output_type":"execute_result","execution_count":14,"time":"Took: 7 seconds 211 milliseconds, at 2017-6-14 16:27"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"D5C2612F7811426C94AD8C33103A6B51"},"cell_type":"code","source":"import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.DenseVector\n\n//import org.apache.spark.ml.linalg.{Vector,Vectors}\n/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 14/06/13\n * Time: 12:34\n * To change this template use File | Settings | File Templates.\n */\n\n class pointObj(\n    val data: DenseVector,//the numeric part of the data-point\n    //val label: Int,            //the real (provided) label\n    val id: Int               //the identifier(=numeroLigne) of the data-point\n    ) extends Serializable {\n  override def toString: String = {\" \"\n    //data.toArray.deep.mkString(\", \") + pointPartBin.toArray.deep.mkString(\", \")\n    /*\"partieNumerique -> \"+pointPartNum.toArray.deep.mkString(\"[\", \", \", \"]\") +\n    \"; partieBinaire -> \"+pointPartBin.toArray.deep.mkString(\"[\", \", \", \"]\")*/ \n  } \n }\n \n\nabstract class AbstractModel(val prototypes: Array[AbstractPrototype]) extends Serializable {\n  def size = prototypes.size\n\n  def findClosestPrototype(data: DenseVector): AbstractPrototype = {\n    prototypes.minBy(proto => proto.dist(data))\n  }\n  \n  def findClosestPrototypeId(data: DenseVector): AbstractPrototype = {\n    prototypes.minBy(proto => proto.dist(data))\n  }  \n\n  def apply(i: Int) = prototypes(i)\n\n  def assign(dataset: RDD[pointObj]): RDD[(Int, Int)] =  {\n    dataset.map(d => (this.findClosestPrototype(d.data).id, d.id))\n  }\n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.DenseVector\ndefined class pointObj\ndefined class AbstractModel\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":15,"time":"Took: 5 seconds 583 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F5437D9C9BB94F9CA5F35AAF31ACC465"},"cell_type":"code","source":":markdown\n#WriterCluster.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res23: String = #WriterCluster.scala\n"},{"metadata":{},"data":{"text/markdown":"#WriterCluster.scala"},"output_type":"execute_result","execution_count":16,"time":"Took: 6 seconds 108 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"B373C55A978C48938C66A7E045AF1AB3"},"cell_type":"code","source":"//package org.lipn.som.utils\nimport org.apache.spark.mllib.linalg.DenseVector\nimport java.io._\nimport org.apache.spark.rdd.RDD\n//import org.lipn.som.global.AbstractModel\n\n\nobject WriterClusters {\n  def js(data: RDD[NamedVector], model: AbstractModel, path: String) = {\n    val writer = new PrintWriter(new File(path))\n\n    val dataArray = data.collect\n    var str = \"var dataset = [\"\n\n    dataArray.foreach {d =>\n      val closestNeuron = model.findClosestPrototype(d)\n      if (d != dataArray.head) str += ','\n      str += d.toJSON(closestNeuron.id)\n    }\n\n    /*model.foreach{proto =>\n      str += ','\n      str += \"{\"\n      for (i <- 0 until proto._point.length) {\n        str += \"attr\"+i+\":\"+proto._point(i)+\", \"\n      }\n      str += \"cls:\\\"proto\\\", \"\n      str += \"clusterId:-\"+proto.id\n      str += \"}\\n\"\n    }\n    */\n    str += \"];\"\n    writer.write(str)\n\n    writer.close()\n  }\n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.linalg.DenseVector\nimport java.io._\nimport org.apache.spark.rdd.RDD\ndefined object WriterClusters\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":17,"time":"Took: 4 seconds 7 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"3F31F7F976564E849122CA88BA83B218"},"cell_type":"code","source":":markdown \n#AbstractTrainer.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res26: String = #AbstractTrainer.scala\n"},{"metadata":{},"data":{"text/markdown":"#AbstractTrainer.scala"},"output_type":"execute_result","execution_count":18,"time":"Took: 3 seconds 842 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"0533032DC3BF449796B196D35705E2ED"},"cell_type":"code","source":"/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 14/06/13\n * Time: 12:31\n * To change this template use File | Settings | File Templates.\n */\n\nimport org.apache.spark.mllib.linalg.DenseVector\nimport java.util.concurrent.TimeUnit._\nimport org.apache.spark.rdd.RDD\nimport scala.concurrent.duration.{FiniteDuration, Duration}\n//import org.apache.spark.ml.linalg.{Vector,Vectors}\n\ntrait AbstractTrainer extends Serializable {\n  private var _it = 0\n  def getLastIt = _it\n\n  private var _converge = 1.0\n  def getLastConvergence = _converge\n\n  private var _trainingDuration = Duration.Zero\n  def getLastTrainingDuration = _trainingDuration\n\n  protected def initModel(dataset: RDD[DenseVector], modelOptions: Map[String, String])\n\n  protected def trainingIteration(dataset: RDD[DenseVector], currentIteration: Int, maxIteration: Int): Double\n\n  protected def getModel: AbstractModel\n\n  final def training(dataset: RDD[DenseVector],\n                     modelOptions: Map[String, String] = Map.empty,\n                     maxIteration: Int = 100,\n                     endConvergeDistance: Double = 0.001): AbstractModel = {\n\n    val datasetSize = dataset.count()\n\n    val startLearningTime = System.currentTimeMillis()\n\n    val model = initModel(dataset, modelOptions)\n    _it = 0\n    _converge = 1.0\n\n    while (_converge > endConvergeDistance && _it < maxIteration) {\n\n      // Training iteration\n      val sumConvergence = trainingIteration(dataset, _it, maxIteration)\n\n      // process convergence\n      _converge = sumConvergence / datasetSize\n      _it += 1\n    }\n\n    _trainingDuration = Duration.create(System.currentTimeMillis() - startLearningTime, MILLISECONDS)\nprintln(\"le model apres training est : \"+getModel)\n\n    // return the model\n    getModel\n  }\n}\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.linalg.DenseVector\nimport java.util.concurrent.TimeUnit._\nimport org.apache.spark.rdd.RDD\nimport scala.concurrent.duration.{FiniteDuration, Duration}\ndefined trait AbstractTrainer\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":19,"time":"Took: 4 seconds 244 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"4AC18190D25842648090D447EC6D777A"},"cell_type":"code","source":":markdown\n#SomTrainerA.scala","outputs":[{"name":"stdout","output_type":"stream","text":"res29: String = #SomTrainerA.scala\n"},{"metadata":{},"data":{"text/markdown":"#SomTrainerA.scala"},"output_type":"execute_result","execution_count":20,"time":"Took: 15 seconds 532 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"6996406D601D42538560B9C882D6B797"},"cell_type":"code","source":"//package org.lipn.som.som\n\nimport scala.math.{abs, exp}\nimport java.util.Random\nimport org.apache.spark.rdd.RDD\n//import org.apache.spark.SparkContext._\n//import org.apache.spark.util.Vector\n//import org.lipn.som.global.{AbstractPrototype, AbstractModel, AbstractTrainer}\n//import org.lipn.som.utils.NamedVector\nimport scala.concurrent.duration.{FiniteDuration, Duration}\n//import org.apache.spark.ml.linalg.{Vector,Vectors}\nimport org.apache.spark.mllib.linalg.DenseVector\n\n/**\n * User: tug\n * Date: 14/06/13\n * Time: 12:35\n */\nclass SomTrainerA extends AbstractTrainer {\n  val DEFAULT_SOM_ROW = 10\n  val DEFAULT_SOM_COL = 10\n  val DEFAULT_TMAX = 8\n  val DEFAULT_TMIN = 1\n  val DEFAULT_INITMAP = 0\n  val DEFAULT_INITMAPFile = \"\"\n  val DEFAULT_SEPARATOR = \"\"\n  val SIZE_REAL_VARS = 10\n  \n  var tmax: Double = DEFAULT_TMAX\n  var tmin: Double = DEFAULT_TMIN\n  var initMap: Int = DEFAULT_INITMAP\n  var initMapFile: String = DEFAULT_INITMAPFile\n  var sep = DEFAULT_SEPARATOR\n  var sizeRealVars: Int = SIZE_REAL_VARS\n \n\n  protected var _somModel: SomModel = null\n  protected def getModel: AbstractModel = _somModel\n\n  protected def initModel(dataset: RDD[DenseVector], modelOptions: Map[String, String]) {\n    var nbRow = DEFAULT_SOM_ROW\n    var nbCol = DEFAULT_SOM_COL\n    if (modelOptions != null) {\n      nbRow = modelOptions(\"clustering.som.nbrow\").toInt\n      nbCol = modelOptions(\"clustering.som.nbcol\").toInt\n      tmax = modelOptions.get(\"clustering.som.tmax\").map(_.toDouble).getOrElse(DEFAULT_TMAX)\n      tmin = modelOptions.get(\"clustering.som.tmin\").map(_.toDouble).getOrElse(DEFAULT_TMIN)\n      initMap = modelOptions.get(\"clustering.som.initMap\").map(_.toInt).getOrElse(DEFAULT_INITMAP)\n      initMapFile = modelOptions.get(\"clustering.som.initMapFile\").map(_.toString).getOrElse(DEFAULT_INITMAPFile)\n      sep = modelOptions.get(\"clustering.som.separator\").map(_.toString).getOrElse(DEFAULT_SEPARATOR)\n      sizeRealVars = modelOptions.get(\"clustering.som.nbRealVars\").map(_.toInt).getOrElse(SIZE_REAL_VARS)\n    }\n\n    val mapSize = nbRow * nbCol\n    // todo : replace random = 42\n    var selectedDatas: Array[DenseVector] = Array()\n    if (initMap == 0) {    \n       selectedDatas = {\n      dataset.takeSample(withReplacement = false, mapSize, new Random().nextInt())\n    }\n    } else {\n       selectedDatas = {\n        scala.io.Source.fromFile(initMapFile).getLines().toArray.map(x => new DenseVector(x.split(sep).map(_.toDouble)))\n      }\n    }\n\n    // todo : Check /nbCol et %nbCOl\n    val neuronMatrix = Array.tabulate(mapSize)(id => new SomNeuron(id, id/nbCol, id%nbCol, selectedDatas(id)))\n    _somModel = new SomModel(nbRow, nbCol, neuronMatrix)\n  }//init model\n\n  protected def trainingIteration(dataset: RDD[DenseVector], currentIteration: Int, maxIteration: Int): Double = {\n    \n    val T = processT(maxIteration, currentIteration)\n\n    // create som observations\n    val mapping = dataset.map{d =>\n      val bestNeuron = _somModel.findClosestPrototype(d).asInstanceOf[SomNeuron]\n      \n      //ML: à rentrer dans la condition\n      var mapBin: scala.collection.immutable.Vector[(Int, Int)] = scala.collection.immutable.Vector()\n      \n      //binary part\n      if (d.size > this.sizeRealVars){\n        val d2: scala.collection.immutable.Vector[Double] = d.toArray.drop(sizeRealVars).toVector.asInstanceOf[scala.collection.immutable.Vector[Double]]\n        mapBin = d2.map(x => if (x == 1) (1,0) else (0,1))\n      }\n\n\n      _somModel.prototypes.map{proto =>\n        val neuron = proto.asInstanceOf[SomNeuron]\n        val factor = neuron.factorDist(bestNeuron, T) // K(delta(.-.)/T)\n             \n        //binary part\n        var mapBinPondere: scala.collection.immutable.Vector[(Double, Double)] = scala.collection.immutable.Vector()\n       \n        //ML:ajouter la condition (d.length > this.sizeRealVars), sinon vecteur vide\n        if (mapBin.size > 0) {\n          mapBinPondere = mapBin.map(x => (x._1 * factor, x._2 * factor))\n        }\n        \n        //ML: dans le cas de non présence de réelle vecteur vide, pareil pour les varibales binaires\n        new SomObsA(new DenseVector(d.toArray.take(sizeRealVars).map(_ * factor)), factor, mapBinPondere, neuron.id)\n        // ligne originale\n        //new SomObsA(Vector(d.toArray.take(sizeRealVars)) * factor, factor, mapBinPondere, neuron.id)\n\n      }\n    } //end mapping\n\n    // Concat observations\n    val concatObs = mapping.reduce{(obs1, obs2) =>\n      for (i <- 0 until obs1.length) {\n        obs1(i) += obs2(i)\n      }\n      obs1\n    }\n\n    // Update model and process convergence distance\n    //val x: Array[Double] = concatObs.map(_somModel.update)\n    concatObs.map(_somModel.update).sum\n    \n  }//end trainingIteration\n\n  //protected def processT(maxIt:Int, currentIt:Int) = maxIt.toFloat - currentIt\n   protected def processT(maxIt:Int, currentIt:Int) =\n      this.tmax*math.pow(this.tmin/this.tmax,currentIt/(maxIt.toFloat-1))\n\n  protected class SomModel(val nbRow: Int, val nbCol: Int, neurons: Array[SomNeuron])\n    extends AbstractModel(neurons.asInstanceOf[Array[AbstractPrototype]]) {\n\n    // Update the data point of the neuron\n    // and return the distance between the new and the old point\n    def update(obs: SomObsA) = neurons(obs.neuronId).update(obs.compute)\n\n\n    override def toString: String = {\n      var str = \"\"\n      for(neuron <- neurons) {\n        str += neuron+\"\\n\"\n      }\n      str\n    }\n  }\n\n  protected class SomNeuron(id: Int, val row: Int, val col: Int, point: DenseVector) extends AbstractPrototype(id, point) {\n    def factorDist(neuron: SomNeuron, T: Double): Double = {\n      exp(-(abs(neuron.row - row) + abs(neuron.col - col)) / T)\n    }\n\n    override def toString: String = {\n      \"(\"+row+\", \"+col+\") -> \"+point\n    }\n  }\n\n  protected class SomObsA(var numerator:DenseVector, var denominator: Double, var mapBinPonderation: scala.collection.immutable.Vector[(Double, Double)], val neuronId: Int) extends Serializable {\n    def +(obs: SomObsA): SomObsA = {\n      //ML:que lorsqu'on a des données réelles\n      numerator = new DenseVector( obs.numerator.toArray.zip(numerator.toArray).map( x => x._1 + x._2 ) )\n      denominator += obs.denominator\n      \n\n      // calcul de la somme des pondÃ©ration des 1 et des 0\n     //ML:ajouter la condition (d.length > this.sizeRealVars)\n      \n      var mapBinPonderation2: scala.collection.immutable.Vector[(Double, Double)] = scala.collection.immutable.Vector()\n    if (mapBinPonderation.size>0)\n      {\n      for (i <-0 until mapBinPonderation.size){\n        val c1: Double = mapBinPonderation(i)._1 + obs.mapBinPonderation(i)._1\n        val c0: Double = mapBinPonderation(i)._2 + obs.mapBinPonderation(i)._2\n         mapBinPonderation2==mapBinPonderation2 :+ (c1, c0)\n      }\n      mapBinPonderation = mapBinPonderation2\n    }\n      \n      this\n    }\n\n    //def compute = numerator / denominator\n    def compute = {\n      // Linge originale\n      //val newPointsReal = numerator / denominator\n      val newPointsReal = new DenseVector( numerator.toArray.map(_ / denominator) )\n      \n      // calcul de la mediane\n      //ML:ajouter la condition (d.length > this.sizeRealVars)\n      //var newPointsBin:Array[Double]=Array()\n      \n      var newPointsBin: scala.collection.immutable.Vector[Double] = scala.collection.immutable.Vector()\n      \n      if (mapBinPonderation.size>0)\n      {\n        newPointsBin = mapBinPonderation.map {e =>\n        if (e._1 >= e._2) 1.0 else 0.0}\n      }\n     \n      // concatenation de la partie real et binaire\n      new DenseVector(newPointsReal.toArray ++ newPointsBin) \n       \n    }\n\n    override def toString = numerator.toString()+\" : \"+denominator.toString\n  }//end SomObsA\n\n\n\n  def purity(dataset: RDD[NamedVector]): Double = {\n    //val nbRealClass = dataset.map(_.cls).reduce(case(cls1,cls2))\n\n    val sumAffectedDatas = dataset.map(d => ((_somModel.findClosestPrototype(d).id, d.cls), 1))\n      .reduceByKey{case (sum1, sum2) => sum1+sum2}\n\n    val maxByCluster = sumAffectedDatas.map(sa => (sa._1._1, sa._2))\n      .reduceByKey{case (sum1, sum2) => sum1.max(sum2) }\n      .map(_._2)\n      .collect()\n\n    maxByCluster.sum / dataset.count().toDouble\n  }\n\n  def affectations(dataset: RDD[NamedVector]): RDD[(Int, Int)] = {\n    dataset.map(d => (d.cls, _somModel.findClosestPrototype(d).id))\n  }\n} //end SomTrainerA\n\n class pointObj(\n    val data: DenseVector,//the numeric part of the data-point\n    //val label: Int,            //the real (provided) label\n    val id: Int               //the identifier(=numeroLigne) of the data-point\n    ) extends Serializable {\n  override def toString: String = {\" \"\n    //data.toArray.deep.mkString(\", \") + pointPartBin.toArray.deep.mkString(\", \")\n    /*\"partieNumerique -> \"+pointPartNum.toArray.deep.mkString(\"[\", \", \", \"]\") +\n    \"; partieBinaire -> \"+pointPartBin.toArray.deep.mkString(\"[\", \", \", \"]\")*/ \n  } \n }\n \n","outputs":[{"name":"stdout","output_type":"stream","text":"import scala.math.{abs, exp}\nimport java.util.Random\nimport org.apache.spark.rdd.RDD\nimport scala.concurrent.duration.{FiniteDuration, Duration}\nimport org.apache.spark.mllib.linalg.DenseVector\ndefined class SomTrainerA\ndefined class pointObj\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":21,"time":"Took: 9 seconds 415 milliseconds, at 2017-6-14 16:28"}]},{"metadata":{"id":"0D9943E8FC48402083C7B5541F4AFD3A"},"cell_type":"markdown","source":"#DataGen"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"C7147CCF43F941E787B71C1AD888EF40"},"cell_type":"code","source":"//package org.lipn.som.utils\nimport org.apache.spark.mllib.linalg.DenseVector\n\n//import util.Random\n//import org.apache.spark.util.Vector\n//import org.apache.spark.SparkContext\nimport org.apache.spark.rdd.RDD\nimport scala.Array\n\n\n/**\n * Created with IntelliJ IDEA.\n * User: tug\n * Date: 27/03/13\n * Time: 17:07\n * To change this template use File | Settings | File Templates.\n */\nobject DataGen extends Serializable {\n\n  class Center(val cls: Int, val rayon: Double, val elements: Array[Double]) extends Serializable {\n    def this(cls: Int, dims: Int, a: Double, b: Double, rayon: Double) = this(cls, rayon, Array.fill(dims)(new Random(42).nextGaussian() * a + b))\n  }\n\n\n  def generate(sc: SparkContext,\n                        numPoints: Int,\n                        nbCls: Int,\n                        d: Int,\n                        numPartitions: Int = 2): RDD[NamedVector] =\n  {\n    // First, generate some centers\n    val rand = new Random(42)\n    val r = 1.0\n    val centers = Array.fill(nbCls)(Array.fill(d)(rand.nextGaussian() * r))\n    // Then generate points around each center\n    sc.parallelize(0 until numPoints, numPartitions).map{ idx =>\n      val cls = idx % nbCls\n      val center = centers(cls)\n      val rand2 = new Random(42 + idx)\n      new NamedVector(Array.tabulate(d)(i => center(i) + rand2.nextGaussian()), cls)\n    }\n  }\n}\n\nobject DataGenerator extends Serializable {\n  private val rand = new Random\n\n  private case class DModel(A: Double, B: Double) {\n    def gen =  A * rand.nextDouble() + B\n  }\n\n  private case class PModel(cls: Int, dmodels: Array[DModel]) {\n    def genVector = new DenseVector(dmodels.map(_.gen))\n    def genNamedVector = new NamedVector(dmodels.map(_.gen), cls)\n  }\n\n  private def PModel2D(cls: Int, A: Double, B: Double, C: Double) = PModel(cls, Array(DModel(A, B), DModel(A, C)))\n\n  private def PModelND(cls: Int, dims: Int, A: Double, B: Double) = PModel(cls, Array.fill(dims)(DModel(A, B)))\n\n  class SModel(N: Int, pmodels: Array[PModel]) {\n    private def nextVector(i: Int) = pmodels(rand.nextInt(pmodels.size)).genVector\n    private def nextNamedVector(i: Int) = pmodels(rand.nextInt(pmodels.size)).genNamedVector\n    def getVector = Array.tabulate(N)(nextVector)\n    def getNamedVector = Array.tabulate(N)(nextNamedVector)\n  }\n  val CLS_1 = 1\n  val CLS_2 = 2\n  val CLS_3 = 3\n  val CLS_4 = 4\n\n  def genH2Dims(N: Int) = new SModel(N, Array(\n    PModel2D(CLS_1, 1, 1, 1),\n    PModel2D(CLS_1, 1, 1, 2),\n    PModel2D(CLS_1, 1, 1, 3),\n    PModel2D(CLS_1, 1, 2, 2),\n    PModel2D(CLS_1, 1, 3, 1),\n    PModel2D(CLS_1, 1, 3, 2),\n    PModel2D(CLS_1, 1, 3, 3)\n  ))\n\n  def gen2Cls2Dims(N: Int) = new SModel(N, Array(\n    PModel2D(CLS_1, 1, 1, 1),\n    PModel2D(CLS_2, 2, 2, 2)\n  ))\n\n  def gen2ClsNDims(N: Int, dims: Int) = new SModel(N, Array(\n    PModelND(CLS_1, dims, 1, 1),\n    PModelND(CLS_2, dims, 2, 2)\n  ))\n}\n\n","outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":24,"time":"Took: 14 seconds 695 milliseconds, at 2017-6-14 16:24"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"E73522D116E34C998FBAFC211BD34F1B"},"cell_type":"code","source":":markdown\n# main with generated data","outputs":[{"metadata":{},"data":{"text/markdown":"# main with generated data"},"output_type":"execute_result","execution_count":25,"time":"Took: 7 seconds 700 milliseconds, at 2017-6-14 16:24"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"4169DCF0BBD34B898E5D688BDE5D8774"},"cell_type":"code","source":" import scala.math.max\n  val nbRowSOM = 10\n  val nbColSOM = 10\n  val nbIter = 30\n  val dataNbObs = 2000\n  val dataNbVars = 10\n  val dataNbCls = 2 //classes\n  val datas = DataGen.generate(sc, dataNbObs, dataNbCls, dataNbVars, max(dataNbObs/10000, 1))\n  datas.cache()\n  datas.count()\n  var startLearningTime = System.currentTimeMillis()\n\n println(\"****************\\n***** SOM  *****\\n****************\")\n  val som = new SomTrainerA\n  val somOptions = Map(\"clustering.som.nbrow\" -> nbRowSOM.toString, \"clustering.som.nbcol\" -> nbColSOM.toString)\n  val somConvergeDist = -0.1\n  startLearningTime = System.currentTimeMillis()\n  val model = som.training(datas.asInstanceOf[RDD[DenseVector]], somOptions, nbIter, somConvergeDist)\n  val somDuration = Duration(System.currentTimeMillis() - startLearningTime, MILLISECONDS)\n\n","outputs":[{"name":"stdout","output_type":"stream","text":"\n"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1173344890-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"8EE2B170C9614FCB80D0ECF21F45D565"},"cell_type":"code","source":"datas.take(1)","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"CAE0F06E43984FB7B9E0A5C18E99DCC8"},"cell_type":"code","source":"object Output extends Serializable {\n  def addHeaderToRdd(sparkCtx: SparkContext, lines: RDD[String], header: String): RDD[String] = {\n    val headerRDD = sparkCtx.parallelize(List((-1L, header)))     // index the header with -1, so that the sort will put it on top.\n    val pairRDD = lines.zipWithIndex()\n    val pairRDD2 = pairRDD.map(t => (t._2, t._1))\n    val allRDD = pairRDD2.union(headerRDD)\n    val allSortedRDD = allRDD.sortByKey()\n    return allSortedRDD.values\n  }\n  def write(outputDir: String, datas: RDD[DenseVector], model: AbstractModel, nbRowSOM:Int, nbColSOM: Int) {\n      val dim = datas.take(1)(0).toArray.length\n      val datasWithIndex = datas.zipWithIndex.map(t => (t._2, t._1))\n    \n      val mapMin = Array.fill[Byte](dim)(0).mkString(\",\")\n      var header = \"# mapDim=2 mapSize={\"+ nbRowSOM +\",\" + nbColSOM + \"}\"\n      header += \" pointDim=\" + dim + \" pointRealDim=\" + dim + \" mapMin={\" + mapMin + \"}\"\n    \n      val prototypes = sc.parallelize(model.prototypes.map(d => (d.id, d._point))).sortByKey().values.collect()\n      println(\"Write Prototypes...\")\n      val protosString = sc.parallelize(prototypes).map(d => d.toArray.mkString(\",\"))\n      val protosResult = addHeaderToRdd(sc, protosString, header)\n      protosResult.coalesce(1).saveAsTextFile(outputDir+\"/maps\")\n\n      val sumAffectedDatas = datas.map(d => (model.findClosestPrototype(d).id, 1))\n        .reduceByKey{case (sum1, sum2) => sum1+sum2}\n        .collectAsMap() \n    \n      // fill in all the prototypes that have 0 observations\n      val card = (0 to prototypes.length - 1).map(d => {\n        if (sumAffectedDatas.contains(d)) {\n          sumAffectedDatas(d) + \"\"\n        } else {\n          \"0\"\n        }\n      })\n    \n      println(\"Write Cardinalities...\")\n      var cardHeader = \"# mapDim=2 mapSize={\"+ nbRowSOM +\",\" + nbColSOM + \"}\" \n      cardHeader +=  \"pointDim=1 pointRealDim=0 mapMin={0} mapMax={0}\"\n      val cardRdd = sc.parallelize(card)\n      val cardResult = addHeaderToRdd(sc, cardRdd, cardHeader)\n      cardResult.coalesce(1).saveAsTextFile(outputDir+\"/cards\")\n      \n    \n      val affHeader = \"# mapDim=1 mapSize={\" + datas.count() + \"} pointDim=1 pointRealDim=0 mapMin={0} mapMax={0}\"\n      val aff = datasWithIndex.map(d => (d._1, model.findClosestPrototype(d._2).id + \"\")).sortByKey().values\n      val affResult = addHeaderToRdd(sc, aff, affHeader)\n      println(\"Write Affiliate...\")\n      affResult.coalesce(1).saveAsTextFile(outputDir+\"/affs\")\n    \n      println(\"Write Maps...\")\n      val maps = sc.parallelize(prototypes.zip(card)).map(d => d._1.toArray.mkString(\",\") + \",\" + d._2)\n        .coalesce(1).saveAsTextFile(outputDir+\"/mapscard\")\n      println(\"Write successfully...\")\n  }\n}\n\n","outputs":[{"name":"stdout","output_type":"stream","text":"defined object Output\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":22,"time":"Took: 7 seconds 810 milliseconds, at 2017-6-14 16:30"}]},{"metadata":{"id":"3FBBA4C01B7948E3960B93E17B8EB19A"},"cell_type":"markdown","source":"#MTM"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"CF974265B6D5481E87519BE77BCE0E2E"},"cell_type":"code","source":"//package org.lipn.som.som\n\n/*import org.lipn.som.global.AbstractModel\nimport org.lipn.som.global.AbstractModel\nimport org.lipn.som.global.AbstractPrototype\nimport org.lipn.som.global.AbstractTrainer\nimport org.lipn.som.utils.NamedVector\nimport org.lipn.som.utils.DataGenerator\n*/\n\n\nobject RunMTM{\n  \n    def main(args:Array[String]) {\n      run(\n          sparkMaster = args(0),\n          intputFile = args(1),\n          outputDir = args(2),\n          execName = args(3),\n          nbRow = args(4).toInt,\n          nbCol = args(5).toInt,\n          tmin = args(6).toDouble,\n          tmax = args(7).toDouble,\n          convergeDist = args(8).toDouble,\n          maxIter = args(9).toInt,\n          sep = args(10),\n          initMap = args(11).toInt, //0: initialisation aleatoire\n          initMapFile = args(12)\n      )  \n    }\n  \n  def run(\n    sparkMaster: String,\n    intputFile: String,\n    outputDir: String,\n    execName: String = \"RunSom\",\n    nbRow: Int = 10, \n    nbCol: Int = 10, \n    tmin: Double = 0.9, \n    tmax: Double = 8,\n    convergeDist: Double = -0.001,\n    maxIter: Int = 50,\n    sep : String = \";\",\n    initMap: Int = 0,\n    initMapFile : String = \"\",\n    nbRealVars : Int = 10\n    ) = {\n//     val sparkConf = new SparkConf().setAppName(execName)\n//     sparkConf.setMaster(sparkMaster)\n//     val sc = new SparkContext(sparkConf)\n\n    val somOptions = Map(\n        \"clustering.som.nbrow\" -> nbRow.toString, \n        \"clustering.som.nbcol\" -> nbCol.toString,\n        \"clustering.som.tmin\" -> tmin.toString,\n        \"clustering.som.tmax\" -> tmax.toString,\n        \"clustering.som.initMap\" -> initMap.toString,\n        \"clustering.som.initMapFile\" -> initMapFile.toString,   \n        \"clustering.som.separator\" -> sep.toString,\n        \"clustering.som.nbRealVars\" -> nbRealVars.toString\n        )\n\n    val trainingDataset = sc.textFile(intputFile).map(x => new DenseVector(x.split(sep).map(_.toDouble))).cache() \n\n    println(s\"nbRow: ${trainingDataset.count()}\")\n    \n    val som = new SomTrainerA\n    val startLearningTime = System.currentTimeMillis()\n    val model = som.training(trainingDataset, somOptions, maxIter, convergeDist)\n    val somDuration = Duration(System.currentTimeMillis() - startLearningTime, MILLISECONDS)\n    \n    Output.write(outputDir, trainingDataset, model, nbRow, nbCol)\n  }\n}\n\n","outputs":[{"name":"stdout","output_type":"stream","text":"defined object RunMTM\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":25,"time":"Took: 7 seconds 340 milliseconds, at 2017-6-14 16:31"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"4619E4C59C7E41AAA549D3EA0B9A20FF"},"cell_type":"code","source":"val sparkMaster = \"MTM\"\nval intputFile = \"/Users/caoquan/MEGA/M1/UP13/Project/work/data/six.txt\"\nval outputDir = \"/Users/caoquan/output\"\nval execName = \"MTM\"\nval nbRow = 5\nval nbCol = 5\nval sep = \" \"\nval tmin: Double = 0.9\nval tmax: Double = 8\nval convergeDist: Double = -0.001\nval maxIter: Int = 50\nval initMap: Int = 0\nval initMapFile : String = \"\"\nval nbRealVars : Int = 10\n\n\nRunMTM.run(\n    sparkMaster,\n    intputFile,\n    outputDir,\n    execName,\n    nbRow, \n    nbCol, \n    tmin, \n    tmax,\n    convergeDist,\n    maxIter,\n    sep,\n    initMap,\n    initMapFile,\n    nbRealVars\n    )","outputs":[{"name":"stdout","output_type":"stream","text":"nbRow: 1800\nle model apres training est : (0, 0) -> [1.00771944243749,-0.865314996104032]\n(0, 1) -> [0.824642028993745,4.71778022828516]\n(0, 2) -> [2.8744597676876,3.43323831361708]\n(0, 3) -> [4.99327598393336,8.28198552221572]\n(0, 4) -> [6.68231326220926,3.85240974569807]\n(1, 0) -> [2.91054691710592,5.95557430617941]\n(1, 1) -> [8.25722435265352,6.41534849255205]\n(1, 2) -> [0.966500015524233,2.52416615333598]\n(1, 3) -> [3.02848091938492,3.02133742518552]\n(1, 4) -> [1.03842620507188,4.98958561649653]\n(2, 0) -> [1.45346792039395,4.98600796089204]\n(2, 1) -> [4.96284531857769,8.23357603980122]\n(2, 2) -> [5.48545561811491,7.01221698781902]\n(2, 3) -> [7.10639041130589,2.97010167943241]\n(2, 4) -> [2.20260777998027,-0.467098489053338]\n(3, 0) -> [7.85084620280884,6.11644976316802]\n(3, 1) -> [1.91896364217629,7.13611436497516]\n(3, 2) -> [9.27394970808058,5.12552473329351]\n(3, 3) -> [4.99337972552049,7.17380958411801]\n(3, 4) -> [8.76285222304729,6.91589202934257]\n(4, 0) -> [2.99814411345934,1.62213803152019]\n(4, 1) -> [8.09426502832056,6.8166647544886]\n(4, 2) -> [3.57991232313949,4.92100077096695]\n(4, 3) -> [1.90054266988982,-0.351031045120238]\n(4, 4) -> [9.82879803904129,6.51622260728569]\n\nWrite Prototypes...\nWrite Cardinalities...\nWrite Affiliate...\nWrite Maps...\nWrite successfully...\nsparkMaster: String = MTM\nintputFile: String = /Users/caoquan/MEGA/M1/UP13/Project/work/data/six.txt\noutputDir: String = /Users/caoquan/output\nexecName: String = MTM\nnbRow: Int = 5\nnbCol: Int = 5\nsep: String = \" \"\ntmin: Double = 0.9\ntmax: Double = 8.0\nconvergeDist: Double = -0.001\nmaxIter: Int = 50\ninitMap: Int = 0\ninitMapFile: String = \"\"\nnbRealVars: Int = 10\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":26,"time":"Took: 42 seconds 49 milliseconds, at 2017-6-14 16:32"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"D7744B91C78B4A3BA478838EC81CCEB1"},"cell_type":"code","source":"","outputs":[]}],"nbformat":4}